{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_class.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPF38VE0PqGPN0viP68ea9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxsolomonhenry/mlp/blob/master/MLP_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVt--gogq8rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo:\n",
        "#       dropout w/ probability.\n",
        "#       shuffle input X.\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class TwoLayerMLP:\n",
        "    def __init__(self, n_iter=1000, learning_rate=0.1, batch_size=30, hidden_layer_width=100,\n",
        "                 hidden_activation='relu'):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_layer_width = hidden_layer_width\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.V = torch.tensor([])\n",
        "        self.W = torch.tensor([])\n",
        "        self.inputs = torch.tensor([])\n",
        "        self.labels = torch.tensor([])\n",
        "        self.X = torch.tensor([])\n",
        "        self.Y = torch.tensor([])\n",
        "        self.Y_hat = torch.tensor([])\n",
        "        self.A1 = torch.tensor([])\n",
        "        self.Z1 = torch.tensor([])\n",
        "        self.Z2 = torch.tensor([])\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperbolic_tangent(Z):\n",
        "        return Z.tanh()\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperbolic_tangent_derivative(Z):\n",
        "        return 1 - Z.tanh() ** 2\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        return 1 / (1 + torch.exp(-Z))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(Z):\n",
        "        return torch.clamp(Z, min=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_subgradient(Z):\n",
        "        return torch.clamp(torch.sign(Z), min=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        # Transposing is to allow for row-wise operations (with built-in broadcasting)\n",
        "        max_dim = 1\n",
        "        if Z.ndim == 1:\n",
        "            max_dim = 0\n",
        "        Z0_transpose = Z.t() - Z.max(max_dim)[0]\n",
        "        Y_hat_transpose = torch.exp(Z0_transpose)\n",
        "        Y_hat_transpose /= Y_hat_transpose.sum(0)\n",
        "        Y_hat = Y_hat_transpose.t()\n",
        "        return Y_hat\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(y_hat, y):\n",
        "        return -torch.dot(y, torch.log(y_hat))\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_grad(y_hat, y):\n",
        "        return -(y * 1 / y_hat) + (1 - y) * (1 / (1 - y_hat))\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_width(tensor):\n",
        "        if len(list(tensor.shape)) == 2:\n",
        "            return tensor.size(1)\n",
        "        elif len(list(tensor.shape)) == 1:\n",
        "            return 1\n",
        "        else:\n",
        "            raise ValueError('Tensor must be one or two dimensions.')\n",
        "\n",
        "    @staticmethod\n",
        "    def matrixify(y):\n",
        "        num_categories = int(y.max()) + 1\n",
        "        num_entries = len(y)\n",
        "        y = y.long()\n",
        "        Y = torch.zeros(num_entries, num_categories)\n",
        "        for i in range(num_entries):\n",
        "            Y[i, y[i]] = 1\n",
        "        return Y\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        return self.sigmoid(Z) * (1 - self.sigmoid(Z))\n",
        "\n",
        "    def softmax_grad(self, Z):\n",
        "        # This code adapted from https://link.medium.com/OgOkKMjCz5\n",
        "        soft_max_result = self.softmax(Z)\n",
        "        reshaped = soft_max_result.view(-1, 1)\n",
        "        return torch.diagflat(reshaped) - torch.ger(reshaped[:, 0], reshaped[:, 0])\n",
        "\n",
        "    def activation(self, Z, output=False, derivative=False):\n",
        "        if not output:\n",
        "            function = self.hidden_activation\n",
        "        else:\n",
        "            function = 'softmax'\n",
        "\n",
        "        if not derivative:\n",
        "            if function == 'relu':\n",
        "                return self.relu(Z)\n",
        "            elif function == 'tanh':\n",
        "                return self.hyperbolic_tangent(Z)\n",
        "            elif function == 'sigmoid':\n",
        "                return self.sigmoid(Z)\n",
        "            elif function == 'softmax':\n",
        "                return self.softmax(Z)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid activation function.\")\n",
        "        else:\n",
        "            if function == 'relu':\n",
        "                return self.relu_subgradient(Z)\n",
        "            elif function == 'tanh':\n",
        "                return self.hyperbolic_tangent_derivative(Z)\n",
        "            elif function == 'sigmoid':\n",
        "                return self.sigmoid_derivative(Z)\n",
        "            elif function == 'softmax':\n",
        "                return self.softmax_grad(Z)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid activation function.\")\n",
        "\n",
        "    def verify_args(self):\n",
        "        if self.batch_size > self.inputs.size(0):\n",
        "            raise ValueError('Mini-batch size cannot be larger than data set.')\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        num_categories = self.tensor_width(self.labels)\n",
        "        num_features = self.inputs.size(1)\n",
        "        self.W = torch.normal(0., 1., [num_features, self.hidden_layer_width])\n",
        "        self.W = self.W.to(self.device)\n",
        "        self.V = torch.normal(0., 1., [self.hidden_layer_width, num_categories])\n",
        "        self.V = self.V.to(self.device)\n",
        "\n",
        "    def set_ith_batch(self, i):\n",
        "        \"\"\"Selects a mini-batch of samples allowing for wraparound.\n",
        "        \"\"\"\n",
        "\n",
        "        first_sample = i * self.batch_size % self.labels.size(0)\n",
        "\n",
        "        if self.batch_size <= (self.inputs.size(0) - first_sample):\n",
        "            self.X = self.inputs[first_sample:first_sample + self.batch_size, :]\n",
        "            self.Y = self.labels[first_sample:first_sample + self.batch_size, :]\n",
        "        else:\n",
        "            input_end = self.inputs[first_sample:, :]\n",
        "            label_end = self.labels[first_sample:, :]\n",
        "            input_wraparound = self.inputs[:(self.batch_size - len(input_end)), :]\n",
        "            label_wraparound = self.labels[:(self.batch_size - len(label_end)), :]\n",
        "            self.X = torch.cat((input_end, input_wraparound))\n",
        "            self.Y = torch.cat((label_end, label_wraparound))\n",
        "        self.X = self.X.to(self.device)\n",
        "        self.Y = self.Y.to(self.device)\n",
        "\n",
        "    def predict_one_batch(self):\n",
        "        # N = number of instances in mini batch\n",
        "        # M = number of hidden units\n",
        "        # C = number of output categories\n",
        "        self.Z1 = torch.mm(self.X, self.W)  # (N x D) x (D x M) -> (N x M)\n",
        "        self.A1 = self.activation(self.Z1)\n",
        "        self.Z2 = torch.mm(self.A1, self.V)  # (N x M) x (M x C) -> (N x C)\n",
        "        self.Y_hat = self.activation(self.Z2, output=True)\n",
        "\n",
        "    def train_one_batch(self):\n",
        "        self.predict_one_batch()\n",
        "        delta_V = 0\n",
        "        delta_W = 0\n",
        "        for i in range(self.batch_size):\n",
        "            # Accumulate gradients for W and V.\n",
        "            loss_and_softmax_grad = self.Y_hat[i, :] - self.Y[i, :]\n",
        "            delta_V += torch.ger(self.A1[i, :], loss_and_softmax_grad)\n",
        "            row_calculation = torch.mv(self.V, loss_and_softmax_grad) * self.activation(self.Z1[i, :], derivative=True)\n",
        "            delta_W += torch.ger(self.X[i, :], row_calculation)\n",
        "        # Average gradients.\n",
        "        delta_V /= self.batch_size\n",
        "        delta_W /= self.batch_size\n",
        "        # Gradient descent.\n",
        "        self.V -= self.learning_rate * delta_V\n",
        "        self.W -= self.learning_rate * delta_W\n",
        "\n",
        "    def train(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.inputs = self.inputs.to(self.device)\n",
        "        self.labels = self.matrixify(labels)\n",
        "        self.labels = self.labels.to(self.device)\n",
        "        self.verify_args()\n",
        "        self.initialize_weights()\n",
        "        for i in range(self.n_iter):\n",
        "            self.set_ith_batch(i)\n",
        "            self.train_one_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKpDFOduwjL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cc579fa5-2f6b-4a9a-a766-919f19a31e49"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mlp = TwoLayerMLP(hidden_activation='relu', n_iter=1000, batch_size=30)\n",
        "\n",
        "num_features = 5\n",
        "num_samples = 300\n",
        "num_labels = 4\n",
        "\n",
        "inputs = torch.rand(num_samples, num_features)\n",
        "labels = (torch.rand(num_samples) * (num_labels - 1)).round()\n",
        "mlp.train(inputs, labels)\n",
        "\n",
        "plt.imshow(mlp.W.to('cpu'))\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLNPG22vwnhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}